{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.feature import graycomatrix, graycoprops, hog\n",
    "import pywt\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "data_dir = \"/raid/home/posahemanth/Phase2/Data\"\n",
    "processed_dir = \"./Processed_Data\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "IMG_SIZE = (224, 224)  # Default size, adjusted for specific models below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images function (unchanged)\n",
    "def load_ct_images(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    classes = os.listdir(dataset_path)\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, IMG_SIZE)\n",
    "                img = img / 255.0\n",
    "                data.append(img)\n",
    "                labels.append(class_name)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_images, train_labels = load_ct_images(os.path.join(data_dir, \"train\"))\n",
    "valid_images, valid_labels = load_ct_images(os.path.join(data_dir, \"valid\"))\n",
    "test_images, test_labels = load_ct_images(os.path.join(data_dir, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images: (613, 224, 224), Train Labels: 613\n",
      "Validation Images: (72, 224, 224), Validation Labels: 72\n",
      "Test Images: (315, 224, 224), Test Labels: 315\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Images: {train_images.shape}, Train Labels: {len(train_labels)}\")\n",
    "print(f\"Validation Images: {valid_images.shape}, Validation Labels: {len(valid_labels)}\")\n",
    "print(f\"Test Images: {test_images.shape}, Test Labels: {len(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for each model\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)), transforms.ToTensor(),  # Inception V3 uses 299x299\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "efficient_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "densenet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "molmo_transform = transforms.Compose([  # Placeholder, adjust if actual specs available\n",
    "    transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/posahemanth/miniconda3/envs/tf_gpu_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/posahemanth/miniconda3/envs/tf_gpu_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/raid/home/posahemanth/miniconda3/envs/tf_gpu_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/raid/home/posahemanth/miniconda3/envs/tf_gpu_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/raid/home/posahemanth/miniconda3/envs/tf_gpu_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained models and adjust for feature extraction\n",
    "# ResNet50\n",
    "resnet50_model = models.resnet50(pretrained=True)\n",
    "resnet50_model = torch.nn.Sequential(*(list(resnet50_model.children())[:-1]))  # Remove fc layer\n",
    "resnet50_model.eval()\n",
    "\n",
    "# Inception V3 - Corrected feature extractor\n",
    "inceptionv3_model = models.inception_v3(pretrained=True)\n",
    "class InceptionFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(InceptionFeatureExtractor, self).__init__()\n",
    "        self.conv2d_1a_3x3 = original_model.Conv2d_1a_3x3\n",
    "        self.conv2d_2a_3x3 = original_model.Conv2d_2a_3x3\n",
    "        self.conv2d_2b_3x3 = original_model.Conv2d_2b_3x3\n",
    "        self.maxpool1 = original_model.maxpool1\n",
    "        self.conv2d_3b_1x1 = original_model.Conv2d_3b_1x1\n",
    "        self.conv2d_4a_3x3 = original_model.Conv2d_4a_3x3\n",
    "        self.maxpool2 = original_model.maxpool2\n",
    "        self.mixed_5b = original_model.Mixed_5b\n",
    "        self.mixed_5c = original_model.Mixed_5c\n",
    "        self.mixed_5d = original_model.Mixed_5d\n",
    "        self.mixed_6a = original_model.Mixed_6a\n",
    "        self.mixed_6b = original_model.Mixed_6b\n",
    "        self.mixed_6c = original_model.Mixed_6c\n",
    "        self.mixed_6d = original_model.Mixed_6d\n",
    "        self.mixed_6e = original_model.Mixed_6e\n",
    "        self.mixed_7a = original_model.Mixed_7a\n",
    "        self.mixed_7b = original_model.Mixed_7b\n",
    "        self.mixed_7c = original_model.Mixed_7c\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))  # Pool to 1x1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1a_3x3(x)\n",
    "        x = self.conv2d_2a_3x3(x)\n",
    "        x = self.conv2d_2b_3x3(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2d_3b_1x1(x)\n",
    "        x = self.conv2d_4a_3x3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.mixed_5b(x)\n",
    "        x = self.mixed_5c(x)\n",
    "        x = self.mixed_5d(x)\n",
    "        x = self.mixed_6a(x)\n",
    "        x = self.mixed_6b(x)\n",
    "        x = self.mixed_6c(x)\n",
    "        x = self.mixed_6d(x)\n",
    "        x = self.mixed_6e(x)\n",
    "        x = self.mixed_7a(x)\n",
    "        x = self.mixed_7b(x)\n",
    "        x = self.mixed_7c(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x\n",
    "\n",
    "inceptionv3_model = InceptionFeatureExtractor(inceptionv3_model)\n",
    "inceptionv3_model.eval()\n",
    "\n",
    "# EfficientNetB0\n",
    "efficientnetb0_model = models.efficientnet_b0(pretrained=True)\n",
    "efficientnetb0_model = torch.nn.Sequential(*(list(efficientnetb0_model.children())[:-1]))  # Remove classifier\n",
    "efficientnetb0_model.eval()\n",
    "\n",
    "# DenseNet121\n",
    "densenet121_model = models.densenet121(pretrained=True)\n",
    "densenet121_model = torch.nn.Sequential(*(list(densenet121_model.children())[:-1]))  # Remove classifier\n",
    "densenet121_model.eval()\n",
    "\n",
    "# Molmo 7B D (Placeholder)\n",
    "molmo7bd_model = models.resnet50(pretrained=True)  # Temporary substitute\n",
    "molmo7bd_model = torch.nn.Sequential(*(list(molmo7bd_model.children())[:-1]))\n",
    "molmo7bd_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "def extract_resnet50_features(image_pil):\n",
    "    image = resnet_transform(image_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = resnet50_model(image)\n",
    "    return features.squeeze().numpy().flatten()\n",
    "\n",
    "def extract_inceptionv3_features(image_pil):\n",
    "    image = inception_transform(image_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = inceptionv3_model(image)\n",
    "    return features.squeeze().numpy().flatten()\n",
    "\n",
    "def extract_efficientnetb0_features(image_pil):\n",
    "    image = efficient_transform(image_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = efficientnetb0_model(image)\n",
    "    return features.squeeze().numpy().flatten()\n",
    "\n",
    "def extract_densenet121_features(image_pil):\n",
    "    image = densenet_transform(image_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = densenet121_model(image)\n",
    "    return features.squeeze().numpy().flatten()\n",
    "\n",
    "def extract_molmo7bd_features(image_pil):  # Placeholder\n",
    "    image = molmo_transform(image_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = molmo7bd_model(image)\n",
    "    return features.squeeze().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-crafted feature extraction (from your original code)\n",
    "def extract_glcm_features(image):\n",
    "    # image_uint8 = (image * 255).astype(np.uint8)\n",
    "    glcm = graycomatrix(image, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
    "    return np.array([contrast, dissimilarity])\n",
    "\n",
    "def extract_wavelet_features(image):\n",
    "    coeffs2 = pywt.dwt2(image, 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    return np.concatenate([LL.flatten(), LH.flatten(), HL.flatten(), HH.flatten()])\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    features, _ = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed feature size (adjust based on combined features)\n",
    "FIXED_FEATURE_SIZE = 3000  # Adjust after testing actual sizes\n",
    "\n",
    "# Preprocess and extract features for all splits\n",
    "def preprocess_and_extract(model_name, extract_deep_fn, transform):\n",
    "    feature_dataset = []\n",
    "    label_dataset = []\n",
    "    \n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "        for class_name in os.listdir(split_dir):\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                \n",
    "                image_pil = Image.open(img_path).convert('RGB')\n",
    "                deep_features = extract_deep_fn(image_pil)\n",
    "                glcm_features = extract_glcm_features(image)\n",
    "                wavelet_features = extract_wavelet_features(image)\n",
    "                hog_features = extract_hog_features(image)\n",
    "                \n",
    "                all_features = np.concatenate([deep_features, glcm_features, wavelet_features, hog_features])\n",
    "                if all_features.shape[0] < FIXED_FEATURE_SIZE:\n",
    "                    all_features = np.pad(all_features, (0, FIXED_FEATURE_SIZE - all_features.shape[0]), mode='constant')\n",
    "                else:\n",
    "                    all_features = all_features[:FIXED_FEATURE_SIZE]\n",
    "                \n",
    "                feature_dataset.append(all_features)\n",
    "                label_dataset.append(class_name)\n",
    "    \n",
    "    features = np.array(feature_dataset, dtype=np.float32)\n",
    "    labels = np.array(label_dataset)\n",
    "    \n",
    "    # Save combined features and labels\n",
    "    np.save(os.path.join(processed_dir, f\"{model_name}_featuresc.npy\"), features)\n",
    "    np.save(os.path.join(processed_dir, f\"{model_name}_labelsc.npy\"), labels)\n",
    "    print(f\"Combined features and labels saved for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all splits and models\n",
    "models_dict = {\n",
    "    \"resnet50\": (extract_resnet50_features, resnet_transform),\n",
    "    \"inceptionv3\": (extract_inceptionv3_features, inception_transform),\n",
    "    \"efficientnetb0\": (extract_efficientnetb0_features, efficient_transform),\n",
    "    \"densenet121\": (extract_densenet121_features, densenet_transform),\n",
    "    \"molmo7bd\": (extract_molmo7bd_features, molmo_transform)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for resnet50...\n",
      "Combined features and labels saved for resnet50\n",
      "Extracting features for inceptionv3...\n",
      "Combined features and labels saved for inceptionv3\n",
      "Extracting features for efficientnetb0...\n",
      "Combined features and labels saved for efficientnetb0\n",
      "Extracting features for densenet121...\n",
      "Combined features and labels saved for densenet121\n",
      "Extracting features for molmo7bd...\n",
      "Combined features and labels saved for molmo7bd\n"
     ]
    }
   ],
   "source": [
    "# Extract features for all models\n",
    "for model_name, (extract_deep_fn, transform) in models_dict.items():\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"{data_dir} doesn't exist!\")\n",
    "        continue\n",
    "    print(f\"Extracting features for {model_name}...\")\n",
    "    preprocess_and_extract(model_name, extract_deep_fn, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate classifier\n",
    "def train_and_evaluate(model_name):\n",
    "    all_features = np.load(os.path.join(processed_dir, f\"{model_name}_featuresc.npy\"))\n",
    "    all_labels = np.load(os.path.join(processed_dir, f\"{model_name}_labelsc.npy\"))\n",
    "\n",
    "    # Train/test split from combined data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        all_features, all_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    train_labels_enc = le.fit_transform(train_labels)\n",
    "    test_labels_enc = le.transform(test_labels)\n",
    "\n",
    "    clf = SVC(kernel='linear', probability=True)\n",
    "    clf.fit(train_features, train_labels_enc)\n",
    "    \n",
    "    test_preds = clf.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels_enc, test_preds)\n",
    "    precision = precision_score(test_labels_enc, test_preds, average='weighted')\n",
    "    recall = recall_score(test_labels_enc, test_preds, average='weighted')\n",
    "    f1 = f1_score(test_labels_enc, test_preds, average='weighted')\n",
    "    \n",
    "    return {\"Model\": model_name, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison Table:\n",
      "         Model  Accuracy  Precision  Recall  F1-Score\n",
      "      resnet50      0.59   0.593986    0.59  0.584051\n",
      "   inceptionv3      0.58   0.577996    0.58  0.573715\n",
      "efficientnetb0      0.57   0.560212    0.57  0.562135\n",
      "   densenet121      0.91   0.909698    0.91  0.909724\n",
      "      molmo7bd      0.59   0.593986    0.59  0.584051\n",
      "Results saved to 'Processed_Data/model_comparisonc.csv'\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models\n",
    "results = []\n",
    "for model_name in models_dict.keys():\n",
    "    result = train_and_evaluate(model_name)\n",
    "    results.append(result)\n",
    "\n",
    "# Generate comparison table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison Table:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(os.path.join(processed_dir, \"model_comparisonc.csv\"), index=False)\n",
    "print(\"Results saved to 'Processed_Data/model_comparisonc.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
